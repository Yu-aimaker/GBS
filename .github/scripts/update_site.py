#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import glob
from datetime import datetime
from bs4 import BeautifulSoup

# è¨­å®š
ARTICLES_DIR = 'articles'
CATEGORIES_DIR = 'categories'
SITEMAP_PATH = 'sitemap.xml'
INDEX_PATH = 'index.html'
BASE_URL = 'https://yu-aimaker.github.io/GBS'

# ã‚«ãƒ†ã‚´ãƒªã¨HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°
CATEGORY_MAPPING = {
    'æœ¬': 'books.html',
    'è‡ªå‹•è»Šãƒ»ãƒã‚¤ã‚¯': 'automotive.html',
    'ãƒ™ãƒ“ãƒ¼ãƒ»ã‚­ãƒƒã‚º': 'baby-kids.html',
    'ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼': 'beauty.html',
    'ãƒ‰ãƒ©ãƒƒã‚°ã‚¹ãƒˆã‚¢': 'drugstore.html',
    'DVDãƒ»CD': 'dvd.html', 
    'å®¶é›»ãƒ»ã‚«ãƒ¡ãƒ©': 'electronics.html',
    'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³': 'fashion.html',
    'é£Ÿå“ãƒ»é£²æ–™': 'food-beverage.html',
    'ã‚²ãƒ¼ãƒ ': 'games.html',
    'ã‚®ãƒ•ãƒˆ': 'gift.html',
    'ãƒ›ãƒ¼ãƒ ãƒ»ã‚­ãƒƒãƒãƒ³': 'home-kitchen.html',
    'ç”£æ¥­ãƒ»ç ”ç©¶é–‹ç™ºç”¨å“': 'industrial.html',
    'ãƒ‘ã‚½ã‚³ãƒ³ãƒ»ã‚ªãƒ•ã‚£ã‚¹ç”¨å“': 'pc-office.html',
    'ãƒšãƒƒãƒˆ': 'pet.html',
    'ã‚¹ãƒãƒ¼ãƒ„ãƒ»ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢': 'sports-outdoor.html',
    'ãŠã‚‚ã¡ã‚ƒãƒ»ãƒ›ãƒ“ãƒ¼': 'toys-hobby.html',
    'æ™‚è¨ˆãƒ»ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼': 'watch-accessories.html',
    'è‡ªå·±ç´¹ä»‹': 'other.html',
    'ãŠçŸ¥ã‚‰ã›': 'other.html',
    'æœªåˆ†é¡': 'other.html'
}

# ãã®ä»–ã‚«ãƒ†ã‚´ãƒªç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
DEFAULT_CATEGORY_FILE = 'other.html'

def get_new_articles():
    """Gitå±¥æ­´ã‹ã‚‰æ–°ã—ã„è¨˜äº‹ã‚’æ¤œå‡ºã™ã‚‹"""
    # ã™ã¹ã¦ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    articles = glob.glob(f'{ARTICLES_DIR}/*.html')
    
    # æ–°ã—ã„è¨˜äº‹ã¨è¦‹ãªã™ï¼ˆå®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§ã¯ã€Gitã®å±¥æ­´ã‚’ä½¿ã£ã¦æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®šã§ãã‚‹ï¼‰
    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã™ã¹ã¦ã®è¨˜äº‹ã‚’å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
    return articles

def extract_article_info(article_path):
    """è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹"""
    with open(article_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f.read(), 'lxml')
    
    # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
    article_id = os.path.basename(article_path).split('-')[0]
    title_tag = soup.find('title')
    title = title_tag.text.strip() if title_tag else "ç„¡é¡Œã®è¨˜äº‹"
    
    # ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
    category = "æœªåˆ†é¡"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    section_meta = soup.find('meta', attrs={'property': 'article:section'})
    if section_meta and section_meta.get('content'):
        category = section_meta.get('content')
    
    # å…¬é–‹æ—¥ã‚’å–å¾—
    published_date = datetime.now().strftime('%Y-%m-%d')
    date_meta = soup.find('meta', attrs={'property': 'article:published_time'})
    if date_meta and date_meta.get('content'):
        try:
            date_str = date_meta.get('content')
            published_date = date_str.split('T')[0]
        except:
            pass
    
    # è¨˜äº‹ã®æ¦‚è¦ã‚’å–å¾—
    description = ""
    desc_meta = soup.find('meta', attrs={'name': 'description'})
    if desc_meta and desc_meta.get('content'):
        description = desc_meta.get('content')
    
    # ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ‘ã‚¹ã‚’å–å¾—
    image_path = ""
    image_meta = soup.find('meta', attrs={'property': 'og:image'})
    if image_meta and image_meta.get('content'):
        image_path = image_meta.get('content').replace(f"{BASE_URL}/", "")
    
    # è¨˜äº‹ã®æœ€åˆã®æ®µè½ï¼ˆè¦ç´„ç”¨ï¼‰
    excerpt = description
    
    return {
        'id': article_id,
        'path': article_path,
        'title': title,
        'category': category,
        'date': published_date,
        'description': description,
        'excerpt': excerpt,
        'image': image_path,
        'url': f'{article_path}'
    }

def update_index_html(articles):
    """index.htmlã®æœ€æ–°è¨˜äº‹ãƒªã‚¹ãƒˆã‚’æ›´æ–°ã™ã‚‹"""
    with open(INDEX_PATH, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f.read(), 'lxml')
    
    # æœ€æ–°ã®æŠ•ç¨¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
    posts_grid = soup.find('div', class_='posts-grid')
    if posts_grid:
        # æ—¢å­˜ã®è¨˜äº‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚¯ãƒªã‚¢
        posts_grid.clear()
        
        # è¨˜äº‹ã‚’æ–°ã—ã„é †ã«ä¸¦ã¹æ›¿ãˆ
        sorted_articles = sorted(articles, key=lambda x: x['id'], reverse=True)
        
        # å„è¨˜äº‹ã®HTMLã‚’ä½œæˆã—ã¦è¿½åŠ 
        for article in sorted_articles:
            # ç”»åƒãƒ‘ã‚¹ã‚’èª¿æ•´ã™ã‚‹
            image_path = article['image']
            
            # BASE_URLã‚’å–ã‚Šé™¤ãï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            if BASE_URL in image_path:
                image_path = image_path.replace(f"{BASE_URL}/", "")
            
            # è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆ../assets/...ï¼‰ã‚’ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆassets/...ï¼‰ã«ä¿®æ­£
            if image_path.startswith('../'):
                image_path = image_path[3:]  # '../'ã‚’å–ã‚Šé™¤ã
            elif image_path.startswith('assets/'):
                # ã™ã§ã«æ­£ã—ã„å½¢å¼ãªã®ã§ãã®ã¾ã¾
                pass
            else:
                # ãã‚Œä»¥å¤–ã®ã‚±ãƒ¼ã‚¹ã§ã¯ã€ãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã¨ã—ã¦æ‰±ã†
                image_path = image_path
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã®HTMLã‚’ä½œæˆ
            article_html = f"""
                <a href="{article['url']}" class="post-preview">
                    <div class="post-layout">
                        <img src="{image_path}" alt="{article['title']}ã®ã‚¢ã‚¤ã‚³ãƒ³ç”»åƒ" class="post-image" width="800" height="200" loading="lazy">
                        <div class="post-content">
                            <h3>{article['title']}</h3>
                            <p class="post-meta">æŠ•ç¨¿æ—¥: {article['date']}</p>
                            <p>{article['excerpt']}</p>
                            <span class="read-more">ç¶šãã‚’èª­ã‚€</span>
                        </div>
                    </div>
                </a>
            """
            # ä½œæˆã—ãŸHTMLã‚’ãƒ‘ãƒ¼ã‚¹
            article_soup = BeautifulSoup(article_html, 'lxml')
            # è¨˜äº‹ãƒªã‚¹ãƒˆã«è¿½åŠ 
            posts_grid.append(article_soup)
    
    # JSONLDãƒ‡ãƒ¼ã‚¿ã‚‚æ›´æ–°
    itemlist_script = soup.find('script', string=re.compile(r'@type": "ItemList"'))
    if itemlist_script:
        # æ–°ã—ã„JSONLDãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        jsonld_items = []
        
        for i, article in enumerate(sorted_articles, 1):
            rel_url = article['url'].replace(ARTICLES_DIR + '/', '')
            image_url = article['image']
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¸è¦ãªéƒ¨åˆ†ã‚’å‰Šé™¤
            title = article['title'].split('|')[0].strip() if '|' in article['title'] else article['title']
            
            item = {
                "@type": "ListItem",
                "position": i,
                "url": f"{BASE_URL}/{article['url']}",
                "name": title,
                "description": article['excerpt'],
                "image": f"{BASE_URL}/{image_url}"
            }
            jsonld_items.append(item)
        
        # JSONLDãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—åŒ–ï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
        jsonld_str = '{\n'
        jsonld_str += '  "@context": "https://schema.org",\n'
        jsonld_str += '  "@type": "ItemList",\n'
        jsonld_str += '  "itemListElement": [\n'
        
        for i, item in enumerate(jsonld_items):
            jsonld_str += '    {\n'
            jsonld_str += f'      "@type": "ListItem",\n'
            jsonld_str += f'      "position": {item["position"]},\n'
            jsonld_str += f'      "url": "{item["url"]}",\n'
            jsonld_str += f'      "name": "{item["name"]}",\n'
            jsonld_str += f'      "description": "{item["description"]}",\n'
            jsonld_str += f'      "image": "{item["image"]}"\n'
            jsonld_str += '    }' + (',' if i < len(jsonld_items) - 1 else '') + '\n'
        
        jsonld_str += '  ]\n'
        jsonld_str += '}\n'
        
        # æ—¢å­˜ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚°ã‚’æ–°ã—ã„JSONLDã§ç½®ãæ›ãˆ
        new_script = soup.new_tag('script')
        new_script['type'] = 'application/ld+json'
        new_script.string = jsonld_str
        itemlist_script.replace_with(new_script)
    
    # æ›´æ–°ã—ãŸHTMLã‚’ä¿å­˜
    with open(INDEX_PATH, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… index.htmlã‚’{len(articles)}ä»¶ã®è¨˜äº‹ã§æ›´æ–°ã—ã¾ã—ãŸ")

def update_category_pages(articles):
    """ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã®è¨˜äº‹ãƒªã‚¹ãƒˆã‚’æ›´æ–°ã™ã‚‹"""
    # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«è¨˜äº‹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    file_articles = {}
    
    # å„è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    for article in articles:
        category_name = article['category']
        # ã‚«ãƒ†ã‚´ãƒªã«å¯¾å¿œã™ã‚‹HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        if category_name in CATEGORY_MAPPING:
            category_file = CATEGORY_MAPPING[category_name]
        else:
            category_file = DEFAULT_CATEGORY_FILE
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã«è¨˜äº‹ã‚’æ•´ç†
        if category_file not in file_articles:
            file_articles[category_file] = []
        file_articles[category_file].append(article)
    
    # å„ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    for category_file, mapped_articles in file_articles.items():
        category_path = os.path.join(CATEGORIES_DIR, category_file)
        
        # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if not os.path.exists(category_path):
            print(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {category_path}, other.htmlã‚’ä½¿ç”¨ã—ã¾ã™")
            category_file = DEFAULT_CATEGORY_FILE
            category_path = os.path.join(CATEGORIES_DIR, DEFAULT_CATEGORY_FILE)
            
            # other.htmlã«è¨˜äº‹ã‚’è¿½åŠ 
            if category_file not in file_articles:
                file_articles[category_file] = []
            file_articles[category_file].extend(mapped_articles)
            continue
        
        # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã‚€
        with open(category_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f.read(), 'lxml')
        
        # è¨˜äº‹ãƒªã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        articles_list = soup.find('div', class_='articles-list')
        if articles_list:
            # æ—¢å­˜ã®è¨˜äº‹ã‚’ã‚¯ãƒªã‚¢
            articles_list.clear()
            
            # è¨˜äº‹ã‚’æ–°ã—ã„é †ã«ä¸¦ã¹æ›¿ãˆ
            sorted_articles = sorted(mapped_articles, key=lambda x: x['id'], reverse=True)
            
            # å„è¨˜äº‹ã®HTMLã‚’ä½œæˆã—ã¦è¿½åŠ 
            for article in sorted_articles:
                # è¨˜äº‹ã®HTMLã‚’ä½œæˆ
                article_html = f"""
                    <article class="article-item">
                        <h3 class="article-title">{article['title'].split('|')[0].strip()}</h3>
                        <div class="article-meta">æŠ•ç¨¿æ—¥: {article['date']}</div>
                        <p class="article-excerpt">{article['description']}</p>
                        <a href="../{article['url']}" class="read-more">ç¶šãã‚’èª­ã‚€</a>
                    </article>
                """
                # ä½œæˆã—ãŸHTMLã‚’ãƒ‘ãƒ¼ã‚¹
                article_soup = BeautifulSoup(article_html, 'lxml')
                # è¨˜äº‹ãƒªã‚¹ãƒˆã«è¿½åŠ 
                articles_list.append(article_soup)
        
        # æ›´æ–°ã—ãŸã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
        with open(category_path, 'w', encoding='utf-8') as f:
            f.write(str(soup))
        
        print(f"âœ… ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {category_path} ({len(mapped_articles)}ä»¶ã®è¨˜äº‹)")

def update_sitemap(articles):
    """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’æ›´æ–°ã™ã‚‹"""
    with open(SITEMAP_PATH, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f.read(), 'xml')
    
    # æ—¢å­˜ã®è¨˜äº‹URLã‚’å–å¾—
    existing_urls = {}
    for url_tag in soup.find_all('url'):
        loc_tag = url_tag.find('loc')
        if loc_tag and "/articles/" in loc_tag.text:
            existing_urls[loc_tag.text] = url_tag
    
    # æ–°ã—ã„è¨˜äº‹ã‚’è¿½åŠ 
    for article in articles:
        article_url = f"{BASE_URL}/{article['url']}"
        
        # ã™ã§ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if article_url in existing_urls:
            continue
        
        # æ–°ã—ã„URLã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
        new_url = soup.new_tag('url')
        
        # locã‚¿ã‚°ã‚’ä½œæˆ
        loc = soup.new_tag('loc')
        loc.string = article_url
        new_url.append(loc)
        
        # lastmodã‚¿ã‚°ã‚’ä½œæˆ
        lastmod = soup.new_tag('lastmod')
        lastmod.string = article['date']
        new_url.append(lastmod)
        
        # changefreqã‚¿ã‚°ã‚’ä½œæˆ
        changefreq = soup.new_tag('changefreq')
        changefreq.string = 'monthly'
        new_url.append(changefreq)
        
        # priorityã‚¿ã‚°ã‚’ä½œæˆ
        priority = soup.new_tag('priority')
        priority.string = '0.9'
        new_url.append(priority)
        
        # ç”»åƒæƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
        if article['image']:
            image = soup.new_tag('image:image', prefix='image')
            
            image_loc = soup.new_tag('image:loc', prefix='image')
            image_loc.string = f"{BASE_URL}/{article['image']}"
            image.append(image_loc)
            
            image_title = soup.new_tag('image:title', prefix='image')
            image_title.string = f"{article['title']}ã®ã‚¢ã‚¤ã‚³ãƒ³ç”»åƒ"
            image.append(image_title)
            
            new_url.append(image)
        
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã«è¿½åŠ 
        soup.urlset.append(new_url)
    
    # æ›´æ–°ã—ãŸã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜
    with open(SITEMAP_PATH, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"âœ… sitemap.xmlã‚’æ›´æ–°ã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ ã‚µã‚¤ãƒˆæ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™...")
    
    # æ–°ã—ã„è¨˜äº‹ã‚’å–å¾—
    article_files = get_new_articles()
    print(f"ğŸ“ å‡¦ç†å¯¾è±¡ã®è¨˜äº‹: {len(article_files)}ä»¶")
    
    # è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
    articles = []
    for article_file in article_files:
        article_info = extract_article_info(article_file)
        articles.append(article_info)
        print(f"âš™ï¸ è¨˜äº‹ã‚’è§£æ: {article_info['title']} (ã‚«ãƒ†ã‚´ãƒª: {article_info['category']})")
    
    # å„ç¨®ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°
    update_index_html(articles)
    update_category_pages(articles)
    update_sitemap(articles)
    
    print("âœ¨ ã‚µã‚¤ãƒˆæ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main() 